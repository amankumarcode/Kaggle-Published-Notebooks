{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://media1.tenor.com/images/187aa2e92e2dbbef3877cd9a8481a91b/tenor.gif?itemid=9005495)","metadata":{}},{"cell_type":"markdown","source":"## Why do we need Mask Detection?\nIn these uncertain times, human civilization needs to quickly adapt to the threat at hand. While wearing mask is not the ultimate solution, it still reduces the rate of transmission of the virus. For us, to create such safe ecosystem, we need techniques like Mask Detection to ensure that the rate of transmission is controlled in highly crowded public spaces and other areas where risk of transmission is high.\n\nIn this notebook, following process is adopted:\n* Step1: Extract the face data for training.\n* Step2: Train the classifier to classify faces into mask or non-mask labels.\n* Step3: Detect Faces in testing data using SSD Face Detector.\n* Step4: By using the trained classifier, classify the detected faces.\n\n## Content\n1.[Importing Libraries and Directories](#section-one)\n2.[What is SSD?](#section-two)\n3.[Functions](#section-three) \n4.[Data Preprocessing](#section-four)\n5.[Model Architecture and Training Process](#section-five)\n6.[Training and Validation Visualizations](#section-six)\n7.[Model Testing](#section-seven)\n8.[Conclusion](#section-eight)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## Importing Libraries and Directories","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\ndirectory = \"../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\nimage_directory = \"../input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\"\ndf = pd.read_csv(\"../input/face-mask-detection-dataset/train.csv\")\ndf_test = pd.read_csv(\"../input/face-mask-detection-dataset/submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## What is SSD?\nSSD is **Single Shot Multibox Detector**. It is a technique that is used to detect objects in images using a single deep neural network. Basically its used for object detection in an image. By using a **base architecture of VGG-16 Architecture**, SSD is able to out perform other object detectors like YOLO and Faster R-CNN in both speed and accuracy. The architecture of SSD is given in the figure below. Training a SSD model from scratch will require a lot of data, so here I have imported pretrained weights **(Caffe Face Detector Model)** using OpenCV.","metadata":{}},{"cell_type":"markdown","source":"![SSD Architecture](https://www.researchgate.net/profile/Adam_Nowosielski/publication/332948824/figure/fig5/AS:767146284036100@1559913335810/The-model-of-Single-Shot-MultiBox-Detector-SSD-25.ppm)","metadata":{}},{"cell_type":"code","source":"cvNet = cv2.dnn.readNetFromCaffe('../input/caffe-face-detector-opencv-pretrained-model/architecture.txt','../input/caffe-face-detector-opencv-pretrained-model/weights.caffemodel')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n## Functions \n1. JSON Function fetches the json file that has the data of bounding box in the training dataset.\n2. Gamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems. In simple terms it is used to instill some light in the image. If gamma < 1, image will shift towards darker end of the spectrum and when gamma > 1, there will be more light in the image.","metadata":{}},{"cell_type":"code","source":"def getJSON(filePathandName):\n    with open(filePathandName,'r') as f:\n        return json.load(f)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_gamma(image, gamma=1.0):\n    invGamma = 1.0 / gamma\n    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## Data Preprocessing ","metadata":{}},{"cell_type":"markdown","source":"Lets look at the JSON data given for training:\n* The Annotations field contains the data of all the faces present in a particular image.\n* There are various classnames but the true classnames are **face_with_mask** and **face_no_mask**.","metadata":{}},{"cell_type":"code","source":"jsonfiles= []\nfor i in os.listdir(directory):\n    jsonfiles.append(getJSON(os.path.join(directory,i)))\njsonfiles[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/face-mask-detection-dataset/train.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* By using the mask label and non_mask label, the bounding box data from json files is extracted.\n* The faces from any particular image are extracted and stored in the data list along with its label for the training process.","metadata":{}},{"cell_type":"code","source":"data = []\nimg_size = 124\nmask = ['face_with_mask']\nnon_mask = [\"face_no_mask\"]\nlabels={'mask':0,'without mask':1}\nfor i in df[\"name\"].unique():\n    f = i+\".json\"\n    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n        if j[\"classname\"] in mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))\n            data.append([img,labels[\"mask\"]])\n        if j[\"classname\"] in non_mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))    \n            data.append([img,labels[\"without mask\"]])\nrandom.shuffle(data)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* The visualization below tells us that the **Number of Mask images > Number of Non-Mask images**, so this is an imbalanced dataset. But since we are using a SSD pretrained model, which is trained to detect non-mask faces, this imbalance would not matter a lot.","metadata":{}},{"cell_type":"code","source":"p = []\nfor face in data:\n    if(face[1] == 0):\n        p.append(\"Mask\")\n    else:\n        p.append(\"No Mask\")\nsns.countplot(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = []\nY = []\nfor features,label in data:\n    X.append(features)\n    Y.append(label)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = np.array(X)/255.0\nX = X.reshape(-1,124,124,3)\nY = np.array(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-five\"></a>\n## Model Architecture and Training Process ","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,    \n        rotation_range=15,    \n        width_shift_range=0.1,\n        height_shift_range=0.1,  \n        horizontal_flip=True,  \n        vertical_flip=False)\ndatagen.fit(xtrain)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n                    steps_per_epoch=xtrain.shape[0]//32,\n                    epochs=50,\n                    verbose=1,\n                    validation_data=(xval, yval))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-six\"></a>\n## Training and Validation Visualizations","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history['accuracy'],'g')\nplt.plot(history.history['val_accuracy'],'b')\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'],'g')\nplt.plot(history.history['val_loss'],'b')\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-seven\"></a>\n## Model Testing ","metadata":{}},{"cell_type":"markdown","source":"The test dataset has 1698 images and to evaluate the model I have taken a handful of images from this dataset as there are no labels for faces in the dataset. ","metadata":{}},{"cell_type":"code","source":"print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Gamma Correction for making the image appear in more light.**(Gamma = 2)**\n* blobFromImage creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels. [Refer OpenCV documentation]\n* The blob is passed through the SSD network and detections are made with some confidence score.\n* Define a threshold confidence score, above which the detection will be considered as a candidate of being a face. (In this case **confidence threshold = 0.2**)\n* All the detections that qualify the confidence score are then passed to the architecture for classification into mask or non-mask image.","metadata":{}},{"cell_type":"code","source":"gamma = 2.0\nfig = plt.figure(figsize = (14,14))\nrows = 3\ncols = 2\naxes = []\nassign = {'0':'Mask','1':\"No Mask\"}\nfor j,im in enumerate(test_images):\n    image =  cv2.imread(os.path.join(image_directory,im),1)\n    image =  adjust_gamma(image, gamma=gamma)\n    (h, w) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    cvNet.setInput(blob)\n    detections = cvNet.forward()\n    for i in range(0, detections.shape[2]):\n        try:\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            frame = image[startY:endY, startX:endX]\n            confidence = detections[0, 0, i, 2]\n            if confidence > 0.2:\n                im = cv2.resize(frame,(img_size,img_size))\n                im = np.array(im)/255.0\n                im = im.reshape(1,124,124,3)\n                result = model.predict(im)\n                if result>0.5:\n                    label_Y = 1\n                else:\n                    label_Y = 0\n                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n        \n        except:pass\n    axes.append(fig.add_subplot(rows, cols, j+1))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-eight\"></a>\n## Conclusion\nBy analyzing the results it can be observed that the whole system works well for faces that have spatial dominance i.e. in image at (1,1), (1,2) and (2,1) but fails in case of (2,2) where the faces are small and occupy less space in the overall image. **To get better results, different image preprocessing techniques can be used, or confidence threshold can be kept lower, or one can try different blob size.**","metadata":{}},{"cell_type":"markdown","source":"If you liked this notebook, then do **Upvote** as it will keep me motivated in creating such kernels ahead. **Thanks!!**","metadata":{}}]}